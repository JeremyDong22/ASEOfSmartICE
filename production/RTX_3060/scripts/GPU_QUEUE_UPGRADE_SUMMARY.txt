================================================================================
GPU QUEUE MANAGEMENT UPGRADE SUMMARY
Version 2.0.0 - 2025-11-14
================================================================================

UPGRADE OVERVIEW:
Replaced simple threading-based parallel processing with intelligent GPU queue
management system. This is NOT just a thread pool - it's a proper job queue
with dynamic concurrency control based on GPU health monitoring.

================================================================================
KEY CHANGES
================================================================================

1. QUEUE-BASED ARCHITECTURE (NOT simple threading)
   - Implemented Python queue.PriorityQueue for job management
   - Jobs prioritized by timestamp (older videos first)
   - Worker threads pull jobs from queue (pool pattern)
   - Maximum concurrent jobs dynamically adjusted based on GPU health

2. GPU HEALTH MONITORING
   - GPUMonitor class queries nvidia-smi every 30 seconds
   - Tracks temperature, utilization, memory usage
   - Three health states: healthy, warm, critical
   - Graceful degradation on macOS (no nvidia-smi)

3. DYNAMIC CONCURRENCY CONTROL
   - Healthy GPU (<70°C): 4 parallel jobs
   - Warm GPU (70-80°C): 2 parallel jobs
   - Hot GPU (>80°C): 1 job at a time
   - Automatic adjustment during processing

4. SMART LOGGING SYSTEM
   - Main log: logs/processing_YYYYMMDD_HHMMSS.log
   - Error log: logs/errors_YYYYMMDD.log (shared daily)
   - Bi-weekly rotation (keeps 14 days)
   - Non-verbose console output
   - Structured format for post-analysis

5. PERFORMANCE METRICS
   - Per-job duration tracking
   - Success/failure statistics
   - Average processing time
   - Queue status monitoring
   - GPU utilization trends

================================================================================
NEW CLASSES
================================================================================

GPUMonitor
----------
- Manages nvidia-smi queries
- Parses GPU metrics (temp, util, memory)
- Determines health status
- Handles cross-platform compatibility

ProcessingJob
-------------
- Represents single video processing task
- Contains: camera_id, video_path, priority, config
- Implements comparison for priority queue ordering

ProcessingQueue
---------------
- Manages job queue and worker threads
- Tracks active workers with thread-safe locking
- Dynamically adjusts parallelism based on GPU health
- Collects processing statistics
- Provides queue status snapshots

================================================================================
NEW COMMAND-LINE OPTIONS
================================================================================

--max-parallel INT
   Maximum parallel jobs on healthy GPU (default: 4)
   Example: --max-parallel 2

--gpu-temp-limit INT
   GPU temperature threshold for minimal parallelism (default: 80°C)
   Example: --gpu-temp-limit 75

--log-level {DEBUG,INFO,WARNING,ERROR}
   Logging verbosity level (default: INFO)
   Example: --log-level DEBUG

================================================================================
BACKWARD COMPATIBILITY
================================================================================

MAINTAINED:
- --videos-dir: Videos directory path
- --cameras: Filter specific cameras
- --duration: Process only N seconds per video
- --config: ROI config file path
- --list: List discovered videos

BEHAVIOR CHANGES:
- Console output now minimal (no verbose per-camera printing)
- All detailed output goes to log files
- Processing order now strictly by timestamp (oldest first)

================================================================================
USAGE EXAMPLES
================================================================================

# Default: Process all videos with GPU queue management
python3 process_videos_orchestrator.py

# Test mode: Process 60s of each video
python3 process_videos_orchestrator.py --duration 60

# Conservative mode: Lower parallelism and temp limit
python3 process_videos_orchestrator.py --max-parallel 2 --gpu-temp-limit 75

# Debug mode: Verbose logging
python3 process_videos_orchestrator.py --log-level DEBUG

# Specific cameras only
python3 process_videos_orchestrator.py --cameras camera_35 camera_22

# List videos without processing
python3 process_videos_orchestrator.py --list

================================================================================
LOG FILES
================================================================================

MAIN PROCESSING LOG (logs/processing_YYYYMMDD_HHMMSS.log)
- All job start/success/failure events
- GPU metrics snapshots (every 30s)
- Queue status updates
- Parallelism adjustments
- Final statistics

ERROR LOG (logs/errors_YYYYMMDD.log)
- Errors only (shared across sessions on same day)
- Easy debugging without searching main log
- Includes error output from failed jobs

LOG ROTATION:
- Automatic cleanup of logs older than 14 days
- Runs on every script startup
- Prevents disk space issues

================================================================================
PERFORMANCE CHARACTERISTICS
================================================================================

GPU MONITORING OVERHEAD:
- nvidia-smi query: ~100ms every 30 seconds
- Negligible impact on processing throughput
- Can be disabled by nvidia-smi unavailability

QUEUE OVERHEAD:
- Job queue operations: O(log n) for priority queue
- Thread synchronization: microseconds with lock
- Minimal compared to video processing (60s+ per job)

SCALABILITY:
- Tested with 12 jobs (3 cameras × 4 videos each)
- Expected to handle 100+ jobs efficiently
- Production workload: 10 cameras × 10 hours = 100 segments

================================================================================
CROSS-PLATFORM SUPPORT
================================================================================

LINUX (Primary deployment):
- Full GPU monitoring via nvidia-smi
- Dynamic concurrency control active
- All features enabled

MACOS (Development):
- GPU monitoring gracefully disabled
- Uses default max_parallel setting
- Queue and logging still functional
- Warning logged: "nvidia-smi not available"

================================================================================
MIGRATION NOTES
================================================================================

FROM v1.0.0:
1. No code changes required in calling scripts
2. Existing command-line args still work
3. Console output format changed (now minimal)
4. Check logs directory after first run
5. Review log files for new format

PRODUCTION DEPLOYMENT:
1. Test with --duration 60 first
2. Monitor GPU temps in logs
3. Adjust --max-parallel if needed
4. Verify log rotation works over 2 weeks
5. Set up log analysis cron jobs

================================================================================
TROUBLESHOOTING
================================================================================

ISSUE: nvidia-smi not found on Linux
SOLUTION: Install NVIDIA drivers, verify with: nvidia-smi
          Script will work but won't adjust parallelism

ISSUE: Too many parallel jobs, GPU overheating
SOLUTION: Lower --max-parallel or --gpu-temp-limit
          System should auto-adjust, but manual override available

ISSUE: Logs directory filling up
SOLUTION: Verify LOG_RETENTION_DAYS in script (default: 14)
          Check cleanup_old_logs() is running on startup

ISSUE: Jobs failing silently
SOLUTION: Check logs/errors_YYYYMMDD.log for error details
          Enable --log-level DEBUG for verbose output

ISSUE: Queue not processing jobs
SOLUTION: Check worker threads started (log shows "Starting N worker threads")
          Verify jobs added to queue (log shows total jobs)
          Check for exceptions in error log

================================================================================
TECHNICAL DETAILS
================================================================================

THREADING MODEL:
- Main thread: Orchestrates setup, starts monitoring
- GPU monitor thread: Background daemon, checks every 30s
- Worker threads: Pool of N workers (default 4)
- Each worker: Pulls jobs from queue, processes sequentially

JOB PRIORITY:
- Priority = timestamp as integer (YYYYMMDDHHMMSS)
- Lower number = older video = higher priority
- Ensures oldest unprocessed videos handled first

CONCURRENCY CONTROL:
- Worker checks active_workers count before pulling job
- If count >= current_parallel, worker sleeps 1s and rechecks
- GPU monitor adjusts current_parallel dynamically
- Changes take effect immediately for next job pull

STATE TRACKING:
- jobs_completed: Successful job count
- jobs_failed: Failed job count
- total_processing_time: Sum of all job durations
- active_workers: Dict mapping thread_id -> current job

================================================================================
FUTURE ENHANCEMENTS
================================================================================

POTENTIAL IMPROVEMENTS:
1. Network quality monitoring during processing
2. Estimated time remaining for queue
3. Progress bar for console output
4. Email/webhook alerts on failures
5. JSON structured logging for parsing
6. Prometheus metrics export
7. Job retry logic for transient failures
8. Per-camera processing time analysis

================================================================================
FILES MODIFIED
================================================================================

CREATED:
+ logs/                                      (directory for log files)
+ scripts/EXAMPLE_LOG_OUTPUT.txt            (log format examples)
+ scripts/GPU_QUEUE_UPGRADE_SUMMARY.txt     (this file)

MODIFIED:
~ scripts/process_videos_orchestrator.py    (v1.0.0 -> v2.0.0)
  - 279 lines -> 768 lines (489 lines added)
  - Added GPUMonitor class
  - Added ProcessingJob class
  - Added ProcessingQueue class
  - Added gpu_monitoring_thread function
  - Added setup_logging function
  - Replaced process_parallel with process_with_queue
  - Updated main() with new args

================================================================================
VERSION HISTORY
================================================================================

v2.0.0 (2025-11-14)
- GPU queue management system
- Smart logging with bi-weekly rotation
- Dynamic concurrency control
- Performance metrics tracking
- Cross-platform support

v1.0.0 (2025-11-14)
- Simple threading-based parallel processing
- One thread per camera
- Basic console output
- No GPU monitoring

================================================================================
CONTACT & SUPPORT
================================================================================

For issues or questions about GPU queue management:
1. Check EXAMPLE_LOG_OUTPUT.txt for log format
2. Review logs/errors_YYYYMMDD.log for errors
3. Enable --log-level DEBUG for verbose output
4. Consult production/RTX_3060/CLAUDE.md for context

Author: ASEOfSmartICE Team
Documentation: /production/RTX_3060/CLAUDE.md
