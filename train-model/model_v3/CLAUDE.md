# Model V3 - YOLOv11 Classification for Waiter/Customer Detection

## Overview
Model V3 uses **YOLOv11 classification** (not detection) to distinguish between waiters and customers from pre-cropped person images. This version leverages high-quality 1080p camera data from camera_35 and camera_22, with transfer learning and aggressive data augmentation for optimal performance.

## Key Features
- **Model Type**: YOLOv11n-cls (Classification - NOT Detection)
- **Camera Selection**: Only camera_35 and camera_22 (1920×1080 resolution)
- **Quality Threshold**: MIN_PERSON_SIZE = 220px (ensures excellent feature clarity)
- **Dataset**: 1,505 manually labeled images (402 waiters, 1,103 customers)
- **Data Quality**: Average 300px person crops (superior to V2's 50-90px)
- **Training Strategy**: Transfer learning + aggressive augmentation for clothing detection
- **Expected Accuracy**: 90-95% (research-backed)

## Camera Statistics

### camera_35 (1920×1080)
- Total images: 1,730
- Average person size: 171×235 pixels
- Status: Most productive camera

### camera_22 (1920×1080)
- Total images: 1,550
- Average person size: 238×166 pixels
- Status: Largest person crops

## Training Pipeline

### 1. Download Raw Images (COMPLETED)
```bash
cd scripts
python3 1_download_from_supabase.py
```
**Status**: ✅ 3,280 images downloaded from Supabase

### 2. Extract Persons (COMPLETED)
```bash
python3 2_extract_persons.py
```
**Status**: ✅ Extracted and randomly sampled 2,000 high-quality person images
- **Threshold**: MIN_PERSON_SIZE = 220px
- **Initial extraction**: 3,972 persons (avg 300px)
- **Random sampling**: Selected 2,000 for labeling convenience
- **Distribution**: camera_22 (1,111), camera_35 (889)

### 3. Label Images
```bash
python3 3_label_images.py
```
Opens web interface at http://localhost:5002 for manual labeling:
- Keyboard shortcuts: Space (toggle), Enter (save & next), ← (previous)
- Default label: Customer
- Output: labeled-persons/waiters/ and labeled-persons/customers/

### 4. Prepare Dataset (CLASSIFICATION FORMAT)
```bash
python3 4_prepare_dataset.py
```
**Status**: Creates ImageFolder structure for YOLO classification (NO label files needed)
- **Format**: dataset/train/{waiter,customer}, dataset/val/{waiter,customer}
- **Split**: 80/20 train/val, stratified by class
- **Output**: Simple folder-based structure, class defined by folder name

### 5. Train Model (YOLOv11 CLASSIFICATION)
```bash
python3 5_train_model.py
```
**Status**: Trains YOLOv11n-cls classification model
- **Model**: YOLOv11n-cls (nano classification model)
- **Device**: MPS (Apple Silicon GPU) with fallback to CPU
- **Training Time**: 2-3 hours on M4 MacBook
- **Optimizer**: SGD with transfer learning from ImageNet
- **Augmentation**: Aggressive (color jitter, rotation, mixup, erasing)
- **Output**: models_cls/waiter_customer_classifier.pt

## Extraction Results & Quality Metrics

### Actual Extraction (Completed)
- **Initial extraction**: 3,972 high-quality persons (220px threshold)
- **Final dataset**: 2,000 persons (randomly sampled for labeling convenience)
- **Quality metrics**:
  - Average min dimension: 300px
  - Median min dimension: 287px
  - Range: 230px - 563px
  - All images ≥220px threshold ✅

### Actual Labeled Dataset
- **Total labeled**: 1,505 images (manually annotated)
- **Class distribution**: 402 waiters, 1,103 customers
- **Training split**: ~1,204 train / ~301 validation (80/20)
- **Expected accuracy**: 90-95% (classification accuracy, not mAP)
- **Quality advantage**: 3-6x better resolution than V2 (300px vs 50-90px)

## Directory Structure
```
model_v3/
├── scripts/              # Training pipeline (5 steps)
│   ├── 1_download_from_supabase.py  # ✅ Camera-filtered download
│   ├── 2_extract_persons.py         # ✅ YOLO person extraction
│   ├── 3_label_images.py            # ✅ Web-based labeling tool
│   ├── 4_prepare_dataset.py         # ✅ Classification format (v3.0)
│   └── 5_train_model.py             # ✅ YOLOv11-cls training (v3.0)
├── raw_images/          # ✅ 3,280 images (camera_35 + camera_22 only)
├── extracted-persons/   # ✅ 2,000+ persons (220px threshold)
│   ├── camera_22/
│   └── camera_35/
├── labeled-persons/     # ✅ 1,505 manually labeled images
│   ├── waiters/         # 402 waiter images
│   │   ├── camera_22/
│   │   └── camera_35/
│   └── customers/       # 1,103 customer images
│       ├── camera_22/
│       └── camera_35/
├── dataset/            # Classification format (generated by step 4)
│   ├── train/
│   │   ├── waiter/     # ~321 images
│   │   └── customer/   # ~882 images
│   ├── val/
│   │   ├── waiter/     # ~81 images
│   │   └── customer/   # ~221 images
│   └── data.yaml       # YOLO classification config
├── models_cls/         # YOLOv11 classification outputs
│   ├── waiter_customer_yolo11n_cls/  # Training artifacts
│   └── waiter_customer_classifier.pt  # Final model
└── configs/            # Configuration files
```

## Key Improvements Over V2

### 1. Correct Model Architecture
- **V2**: Used YOLO detection (unnecessary - images already cropped)
- **V3**: Uses YOLO classification (correct for this task)
- **Result**: Clearer metrics, faster training, no bounding box confusion

### 2. Better Image Quality
- **Resolution**: 1080p only (camera_35 + camera_22)
- **Person Size**: Average 300px vs V2's 50-90px
- **Visibility**: Uniform colors, logos, and features clearly visible

### 3. Research-Backed Training
- **Transfer Learning**: Leverages ImageNet pre-training
- **Data Augmentation**: Aggressive augmentation (3x effective dataset size)
- **Optimization**: SGD + proper learning rate scheduling
- **Device**: MPS acceleration on Apple Silicon

## Training Configuration (YOLOv11-cls)

- **Model**: YOLOv11n-cls (classification nano)
- **Device**: MPS (Apple Silicon GPU) / fallback to CPU
- **Optimizer**: SGD with momentum=0.9
- **Learning Rate**: 0.001 → 0.0001 (fine-tuning)
- **Epochs**: 50 (early stopping patience=10)
- **Batch Size**: 16 (optimal for M4)
- **Image Size**: 224×224 (classification standard)
- **Workers**: 8 (utilize M4 cores)

### Data Augmentation Strategy
- **Color**: HSV jitter (hue=0.015, sat=0.1, val=0.2)
- **Geometric**: Rotation ±15°, horizontal flip 50%
- **Advanced**: MixUp 20%, random erasing 10%
- **Purpose**: Triple effective dataset size for clothing detection

## Model Usage

```python
from ultralytics import YOLO

# Load trained classifier
model = YOLO('models_cls/waiter_customer_classifier.pt')

# Classify a person image
results = model('person_crop.jpg')

# Get prediction
class_id = results[0].probs.top1      # 0=customer, 1=waiter (alphabetical folder order)
confidence = results[0].probs.top1conf  # Confidence score
class_name = results[0].names[class_id]  # 'waiter' or 'customer'

print(f"Prediction: {class_name} ({confidence:.2%})")
```

## Next Steps After Training

1. Validate model on held-out test set
2. Analyze confusion matrix for class-specific errors
3. Deploy to production environment (ONNX export available)
4. Monitor real-world performance and collect edge cases
5. Consider expanding dataset if accuracy < 90%
